{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+HnYu6Fw3ZrtKPYIXTAtl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamthedoan/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8FIAsoU44TEl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ua3a9yp6AOL",
        "outputId": "47106d7c-cbae-4dba-c86a-604976da7fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-04 06:21:56--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-04 06:21:56 (1.74 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bjyOKVa6ETH",
        "outputId": "4cab78c6-6346-4e7b-9089-fd67130c6e31"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyqDZYdZ6Idw",
        "outputId": "928f1a5f-0568-403b-98b9-fea8ac408aaa"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(len(w) for w in words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbALn7cH6Nco",
        "outputId": "4df37787-2ec1-4c86-dc31-a4b0bc2663cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(len(w) for w in words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ETc1X4e6P8n",
        "outputId": "eca49360-3262-4bd8-97ae-c6f22319b687"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?**"
      ],
      "metadata": {
        "id": "LM4ILxem6viu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of chars\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "chars = [\".\"] + chars\n",
        "\n",
        "# dict mapping char to index\n",
        "stoi = {s:i for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "# dict mapping index to char\n",
        "itos = {i:s for s,i in stoi.items()}\n"
      ],
      "metadata": {
        "id": "B83Sc1uX6wQX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stoi)"
      ],
      "metadata": {
        "id": "xJtaSoQr6Sdb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(itos)"
      ],
      "metadata": {
        "id": "JMHi0VjH8e44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = {}\n",
        "for w in words[:5]:\n",
        "  # add start and end tokens\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    trigram = (ch1,ch2,ch3)\n",
        "    t[trigram] = t.get(trigram, 0) + 1"
      ],
      "metadata": {
        "id": "DmLFqMRc8guz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(t.items(), key = lambda kv: -kv[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ua910WHZ9vza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting"
      ],
      "metadata": {
        "id": "2diMhIbKCMjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.ones(27,27,27, dtype=torch.int32)\n",
        "# , device = device\n",
        "\n",
        "N[0,0,0] = 0\n",
        "\n",
        "# counting the trigrams\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    N[ix1, ix2, ix3] += 1\n",
        "\n",
        "# P is N but all values are probabilities\n",
        "P = N / N.sum(dim = 2, keepdim = True)"
      ],
      "metadata": {
        "id": "nu540tiB-cMr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs92UHdg-z-M",
        "outputId": "1a8532f6-2f6f-477f-95ec-1e64cc84897f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27, 27, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "F7vTwZZcCPpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(input):\n",
        "  log_likelihood = 0.0\n",
        "\n",
        "  # for calculating average\n",
        "  n = 0\n",
        "\n",
        "  # for w in [\"brandon\"]\n",
        "\n",
        "  for w in input:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3, in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "      prob = P[ix1, ix2, ix3]\n",
        "      # print(f'{ch1}{ch2}{ch3}: {prob:.4f}')\n",
        "\n",
        "      logprob = torch.log(prob)\n",
        "      log_likelihood += logprob\n",
        "      n += 1\n",
        "\n",
        "  # higher log_likelihood better, log value closer to 0 means probability was higher\n",
        "  print(f'{log_likelihood=}')\n",
        "\n",
        "  # negative log likelihood\n",
        "  nll = -log_likelihood\n",
        "  print(f'{nll=}')\n",
        "\n",
        "  # normalized negative log likelihood, minimize this\n",
        "  print(f'{nll/n}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKE-Vdi9AGmq",
        "outputId": "1483ca78-83c7-4a5a-8af3-e4df608a5ca2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-6.2656)\n",
            "nll=tensor(6.2656)\n",
            "2.088524103164673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyJncSHdHMFY",
        "outputId": "925dd21f-4cc2-49d8-cf21-8cba5edda7ad"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-410414.9688)\n",
            "nll=tensor(410414.9688)\n",
            "2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling"
      ],
      "metadata": {
        "id": "jG9pfOn3E9k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "\n",
        "for i in range(10):\n",
        "  out = []\n",
        "  ix1,ix2 = 0,0\n",
        "  while True:\n",
        "    p = P[ix1, ix2]\n",
        "    ix1 = ix2\n",
        "    ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
        "    if ix2 == 0:\n",
        "      break\n",
        "    out.append(itos[ix2])\n",
        "\n",
        "  names.append(\"\".join(out))\n",
        "\n",
        "print(names)\n",
        "loss_func(names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbPn56YVFBDU",
        "outputId": "015a1bd7-2729-4d29-85d4-0985f66920ce"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['zur', 'yelays', 'karigha', 'yrene', 'yariola', 'glce', 'trah', 'ulennakinn', 'xashadhir', 'daishia']\n",
            "log_likelihood=tensor(-130.2120)\n",
            "nll=tensor(130.2120)\n",
            "2.100193977355957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi layer perceptron (MLP) approach**\n"
      ],
      "metadata": {
        "id": "CA80Se3_H7Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up inputs and outputs"
      ],
      "metadata": {
        "id": "WkrMMia4Yx5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create training set of bigrams (x,y)\n",
        "\n",
        "xs, ys = [],[]\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    # print(ch1, ch2, ch3)\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs, dtype = torch.int64)\n",
        "ys = torch.tensor(ys, dtype = torch.int64)"
      ],
      "metadata": {
        "id": "m0bXuiBHIA0i"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create NN–perform forward pass, softmax, loss, backward pass, update grad"
      ],
      "metadata": {
        "id": "TzqS1f5tY0kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights\n",
        "W = torch.randn((27*2), 27, requires_grad=True)\n",
        "\n",
        "num_pass = 200\n",
        "\n",
        "for i in range(num_pass):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "  # softmax, used for normalizing output to a probability distribution\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = torch.exp(logits) # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim = 1, keepdims = True) # probabilites for next character\n",
        "\n",
        "  # loss (negative log likelihood)\n",
        "  loss = -probs[torch.arange(len(xs)),ys].log().mean()\n",
        "\n",
        "  # regularization: incentivizes W to be near 0 --> smoothing\n",
        "  loss += 0.2 * (W**2).mean()\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(f\"{i}: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set grad to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "      W -= 50 * W.grad\n",
        "\n",
        "  # W.data += -50 * W.grad\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9Y-BrKSI837",
        "outputId": "586bc249-68db-43cf-ebb1-02822d26f4a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.3992\n",
            "10: 2.4116\n",
            "20: 2.2584\n",
            "30: 2.2016\n",
            "40: 2.1737\n",
            "50: 2.1583\n",
            "60: 2.1491\n",
            "70: 2.1434\n",
            "80: 2.1398\n",
            "90: 2.1374\n",
            "100: 2.1357\n",
            "110: 2.1346\n",
            "120: 2.1338\n",
            "130: 2.1333\n",
            "140: 2.1329\n",
            "150: 2.1326\n",
            "160: 2.1324\n",
            "170: 2.1322\n",
            "180: 2.1321\n",
            "190: 2.1320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling"
      ],
      "metadata": {
        "id": "RbdWPxYWY-ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "\n",
        "for i in range(10):\n",
        "  out = []\n",
        "  ix1,ix2 = 0,0\n",
        "  while True:\n",
        "    xenc = F.one_hot(torch.tensor([ix1,ix2]), num_classes = 27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    p = counts / counts.sum(1, keepdims = True)\n",
        "\n",
        "    ix1 = ix2\n",
        "    ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
        "    out.append(itos[ix2])\n",
        "    if ix2 == 0:\n",
        "      break\n",
        "\n",
        "  names.append(\"\".join(out))\n",
        "\n",
        "print(names)\n",
        "loss_func(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylEzLv4FYqRS",
        "outputId": "84c5e5a1-5ee5-498b-c2e4-bb037f760db5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ouray.', 'aivak.', 'xa.', 'umysha.', 'chzmayla.', 'arber.', 'alyalas.', 'ir.', 'aan.', 'eryor.']\n",
            "log_likelihood=tensor(-155.0886)\n",
            "nll=tensor(155.0886)\n",
            "2.6739416122436523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?**"
      ],
      "metadata": {
        "id": "WTrFZnFog8En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(words, test_size=0.2)\n",
        "dev_set, test_set = train_test_split(test_set, test_size=0.5)\n",
        "\n",
        "x_train, y_train, x_dev, y_dev, x_test, y_test = [],[],[],[],[],[]\n",
        "\n",
        "for dataset in [train_set, dev_set, test_set]:\n",
        "  xs, ys = [], []\n",
        "\n",
        "  for w in dataset:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "\n",
        "      xs.append([ix1, ix2])\n",
        "      ys.append(ix3)\n",
        "\n",
        "  xs = torch.tensor(xs, dtype = torch.int64)\n",
        "  ys = torch.tensor(ys, dtype = torch.int64)\n",
        "\n",
        "  if dataset == train_set:\n",
        "    x_train, y_train = xs, ys\n",
        "  elif dataset == dev_set:\n",
        "    x_dev, y_dev = xs, ys\n",
        "  else:\n",
        "    x_test, y_test = xs, ys\n"
      ],
      "metadata": {
        "id": "ixb-N9u_hRq2"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKoSKxl_L6n",
        "outputId": "c2cb1d99-cd95-43f3-f79a-f90c0eb7f6b5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([19568, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same code as above but x and y are now the x_train and y_train\n",
        "\n",
        "# weights\n",
        "W = torch.randn((27*2), 27, requires_grad=True)\n",
        "\n",
        "num_pass = 200\n",
        "\n",
        "for i in range(num_pass):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(x_train, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "  # softmax, used for normalizing output to a probability distribution\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = torch.exp(logits) # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim = 1, keepdims = True) # probabilites for next character\n",
        "\n",
        "  # loss (negative log likelihood)\n",
        "  loss = -probs[torch.arange(len(x_train)),y_train].log().mean()\n",
        "\n",
        "  # regularization: incentivizes W to be near 0 --> smoothing\n",
        "  # loss += 0.2 * (W**2).mean()\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(f\"{i}: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set grad to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "      W -= 50 * W.grad\n",
        "\n",
        "  # W.data += -50 * W.grad\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVGv3w268aMA",
        "outputId": "4f67c6b6-ca32-46e7-bffa-90a40fc959b3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.3586\n",
            "10: 2.4827\n",
            "20: 2.3676\n",
            "30: 2.3243\n",
            "40: 2.3019\n",
            "50: 2.2885\n",
            "60: 2.2794\n",
            "70: 2.2729\n",
            "80: 2.2679\n",
            "90: 2.2640\n",
            "100: 2.2609\n",
            "110: 2.2584\n",
            "120: 2.2563\n",
            "130: 2.2545\n",
            "140: 2.2530\n",
            "150: 2.2517\n",
            "160: 2.2506\n",
            "170: 2.2496\n",
            "180: 2.2487\n",
            "190: 2.2479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[0]"
      ],
      "metadata": {
        "id": "KBtmWN0IDJCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M_uW9ibwDI0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func_MLP(x, y, W):\n",
        "  xenc = F.one_hot(x, num_classes = 27).float()\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims = True)\n",
        "\n",
        "  loss = -probs[torch.arange(len(x)),y].log().mean()\n",
        "\n",
        "  return loss.item()\n"
      ],
      "metadata": {
        "id": "ivSA-ItI7pcX"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train loss: {loss_func_MLP(x_train, y_train, W):.4f}\")\n",
        "print(f\"dev loss: {loss_func_MLP(x_dev, y_dev, W):.4f}\")\n",
        "print(f\"test loss: {loss_func_MLP(x_test, y_test, W):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6rzvp4U6wKo",
        "outputId": "faeb10a9-bf03-4f14-8831-60efc09ca960"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 2.2472\n",
            "dev loss: 2.2523\n",
            "test loss: 2.2518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?**"
      ],
      "metadata": {
        "id": "w-9ZI625AiDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights\n",
        "W = torch.randn((27*2), 27, requires_grad=True)\n",
        "\n",
        "\n",
        "for i in range(200):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(x_train, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "  # softmax, used for normalizing output to a probability distribution\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = torch.exp(logits) # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim = 1, keepdims = True) # probabilites for next character\n",
        "\n",
        "  # loss (negative log likelihood)\n",
        "  loss = -probs[torch.arange(len(x_train)),y_train].log().mean()\n",
        "\n",
        "  # regularization: incentivizes W to be near 0 --> smoothing\n",
        "  # loss += 0.05 * (W**2).mean()\n",
        "  # best with no regularization\n",
        "\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(f\"{i}: train loss {loss.item():.4f} | dev loss {loss_func_MLP(x_dev, y_dev, W):.4f}\")\n",
        "\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set grad to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "      W -= 50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jla2DaXZAkrc",
        "outputId": "52645d36-46b2-4a48-db18-ebf672f343bc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: train loss 4.1880 | dev loss 4.1859\n",
            "10: train loss 2.5002 | dev loss 2.5088\n",
            "20: train loss 2.3823 | dev loss 2.3905\n",
            "30: train loss 2.3345 | dev loss 2.3418\n",
            "40: train loss 2.3085 | dev loss 2.3151\n",
            "50: train loss 2.2927 | dev loss 2.2987\n",
            "60: train loss 2.2823 | dev loss 2.2881\n",
            "70: train loss 2.2751 | dev loss 2.2807\n",
            "80: train loss 2.2698 | dev loss 2.2753\n",
            "90: train loss 2.2657 | dev loss 2.2711\n",
            "100: train loss 2.2625 | dev loss 2.2679\n",
            "110: train loss 2.2598 | dev loss 2.2652\n",
            "120: train loss 2.2576 | dev loss 2.2630\n",
            "130: train loss 2.2558 | dev loss 2.2612\n",
            "140: train loss 2.2542 | dev loss 2.2596\n",
            "150: train loss 2.2528 | dev loss 2.2583\n",
            "160: train loss 2.2516 | dev loss 2.2571\n",
            "170: train loss 2.2505 | dev loss 2.2560\n",
            "180: train loss 2.2496 | dev loss 2.2551\n",
            "190: train loss 2.2487 | dev loss 2.2543\n"
          ]
        }
      ]
    }
  ]
}