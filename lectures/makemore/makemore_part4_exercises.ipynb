{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamthedoan/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "x6GhEWW18aCS",
        "outputId": "e6485cc5-781c-41cc-aeb0-10b7f4bf0741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-10 18:42:28--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-10 18:42:28 (6.65 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "klmu3ZG08PPr",
        "outputId": "126199d6-aa2c-462c-ff7b-b36e2747f590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BCQomLE_8PPs",
        "outputId": "8a3acbb9-a2b7-449d-a853-f852335f2233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V_zt2QHr8PPs",
        "outputId": "ba421a1d-85ec-4f3c-f701-76ffc3ae5340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZlFLjQyT8PPu",
        "outputId": "b678773a-0942-4747-df5a-dbd2b2820348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8ofj1s6d8PPv",
        "outputId": "79940073-e361-4c85-c6d4-0ca572bd46cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3464, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer | b2 broadcasts, is a row vector and is replicated vertically\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values # returns max values and their indices\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv # implicit broadcasting aligning the tensors\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
      ],
      "metadata": {
        "id": "NRukL4jokZWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2.shape"
      ],
      "metadata": {
        "id": "_H4ruuRHMMHc",
        "outputId": "4fa547e5-57f2-4d1d-b172-7e8bfd06c45f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mO-8aqxK8PPw",
        "outputId": "3dc347ff-7ba1-43e1-ef76-ca753ac27275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "W1              | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
            "b1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "\n",
        "# if broadcasting in one pass, sum in other pass\n",
        "\n",
        "# dlogprobs should be same size as logprobs: torch.Size([32,27])\n",
        "dlogprobs = torch.zeros_like(logprobs) # creates tensor of zeros in the same shape as logprobs\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "\n",
        "dprobs = (1.0/probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # need to sum the grad bc its used multiple times\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum # second branch, bc counts influences two downstream nodes\n",
        "dnorm_logits = counts * dcounts # counts is norm_logits.exp()\n",
        "dlogits = dnorm_logits.clone() # makes a clone, not the final derivative\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # logit_maxes is a column tensor\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # routing dlogit_maxes to only the max values\n",
        "\n",
        "# matrix multiplication\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "\n",
        "# BatchNorm layer\n",
        "dhpreact = (1.0 - h ** 2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0,keepdim =True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar # automatically does broadcasting which replciates the values over\n",
        "dbndiff += 2*bndiff * dbndiff2 # second branch of bndiff\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0) # first portion of the derivative\n",
        "dhprebn += 1.0/n * torch.ones_like(hprebn) * dbnmeani # torch ones like broadcasts\n",
        "\n",
        "# forward pass\n",
        "# use the shapes to figure out the matrix multiplication\n",
        "dembcat = dhprebn @ W1.T # hbprebn = embcat @ W1 + b1\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0) # eliminate the 32 dimension\n",
        "# embedding\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C) # creates 27,10 tensor of all C's\n",
        "for k in range(Xb.shape[0]): # iterates through all integers in Xb\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j] # summing the derivatives\n",
        "\n",
        "\n",
        "# -----------------\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)\n",
        "\n",
        "# values not exact bc running on colab, should be exact if running locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebLtYji_8PPw"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-gCXbB4C8PPx",
        "outputId": "15741b41-b414-4a45-e2f7-c374e8109191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "# want derivative of loss with respect to ith logit\n",
        "dlogits = F.softmax(logits,1) # softmax along the row\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n # dlogits[0].sum() = 0\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap=\"gray\")\n",
        "# dlogits is the prob matrix in the forward pass\n",
        "# black squares are indices where we substracted 1\n",
        "\n",
        "# pull down prob of incorrect characters, pull up prob of correct char\n",
        "# amount of push and pull is equal, bc sum of a row is 0\n",
        "# amount of force is proprortional to prob in forward pass"
      ],
      "metadata": {
        "id": "4G6n7gtnb_jp",
        "outputId": "9b8fa1d3-3ec8-41f3-8515-2461ab68fc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bf82dff5d50>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd-MkhB68PPy"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "POdeZSKT8PPy",
        "outputId": "6dc2e4af-f46d-42eb-e1c4-ad5b09f4c891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wPy8DhqB8PPz",
        "outputId": "b1f41c44-fc02-4d62-c1df-405c04c2a7f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7936\n",
            "  10000/ 200000: 2.1552\n",
            "  20000/ 200000: 2.3943\n",
            "  30000/ 200000: 2.4629\n",
            "  40000/ 200000: 1.9642\n",
            "  50000/ 200000: 2.4094\n",
            "  60000/ 200000: 2.4020\n",
            "  70000/ 200000: 2.0456\n",
            "  80000/ 200000: 2.3160\n",
            "  90000/ 200000: 2.1166\n",
            " 100000/ 200000: 1.9706\n",
            " 110000/ 200000: 2.3981\n",
            " 120000/ 200000: 1.9889\n",
            " 130000/ 200000: 2.3964\n",
            " 140000/ 200000: 2.3602\n",
            " 150000/ 200000: 2.1858\n",
            " 160000/ 200000: 1.9414\n",
            " 170000/ 200000: 1.7762\n",
            " 180000/ 200000: 1.9633\n",
            " 190000/ 200000: 1.9651\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    # YOUR CODE HERE :)\n",
        "    # cross entropy\n",
        "    dlogits = F.softmax(logits,1) # softmax along the row\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # second layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h ** 2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T # hbprebn = embcat @ W1 + b1\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0) # eliminate the 32 dimension\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C) # creates 27,10 tensor of all C's\n",
        "    for k in range(Xb.shape[0]): # iterates through all integers in Xb\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j] # summing the derivatives\n",
        "\n",
        "\n",
        "    # dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #   break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossi = []\n",
        "stepi = []\n",
        "\n",
        "stepi.append(i)\n",
        "lossi.append(loss.log10().item())\n",
        "\n",
        "plt.plot(stepi, lossi)\n",
        "# can add these lines above to see how loss changes"
      ],
      "metadata": {
        "id": "MyB0q3Pclgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZEpI0hMW8PPz",
        "outputId": "19d224c1-1d74-4263-e87a-0be880cb50b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
          ]
        }
      ],
      "source": [
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6aFnP_Zc8PP0",
        "outputId": "be5761a0-31b2-44e0-896e-5d676af72b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.070500612258911\n",
            "val 2.1100051403045654\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xHeQNv3s8PP1",
        "outputId": "fe371289-57de-43bc-ef74-bb65c7429f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "mayah.\n",
            "see.\n",
            "madhayla.\n",
            "renvregendraegustered.\n",
            "elin.\n",
            "shi.\n",
            "jenleigh.\n",
            "estanar.\n",
            "kayzion.\n",
            "kamin.\n",
            "shubergiaz.\n",
            "jest.\n",
            "jair.\n",
            "jennex.\n",
            "terorius.\n",
            "macder.\n",
            "yarue.\n",
            "els.\n",
            "kayshustella.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}