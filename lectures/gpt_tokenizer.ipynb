{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHWrh6OkYhyDF6u2p9GPO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamthedoan/nn-zero-to-hero/blob/master/lectures/gpt_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a GPT-4 Tokenizer**\n",
        "\n"
      ],
      "metadata": {
        "id": "1lWnwvjO60V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken # added for colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh7yVd-GmkzL",
        "outputId": "2ba9c415-500e-428d-ce95-5c669175e53a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xHtxV4ymUel",
        "outputId": "7b225f58-2ad1-4a98-ea98-2b9b20141e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31495, 230, 75265, 243, 92245, 62904, 233, 320, 15339, 304, 16526, 16715]\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # GPT-4 tokenizer\n",
        "print(enc.encode(\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\"))\n",
        "print(enc.decode(enc.encode(\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\")) == \"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\")\n",
        "# match the above for your own tokenizer, and also implement a train() function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En2bKE7KOhvp"
      },
      "source": [
        "### minbpe exercise\n",
        "\n",
        "At this point you have everything you need to build your own GPT-4 tokenizer. This is the [exercise progression](https://github.com/karpathy/minbpe/blob/master/exercise.md) you may wish to follow. You'll note that it is part of the [minbpe](https://github.com/karpathy/minbpe) repo, which is the solution to that exercise, and is a cleaned up version of the code above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1MhDpz7zmo",
        "outputId": "ca64002d-45e0-454f-9996-b7f55770ed82"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 23:30:46--  https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185768 (181K) [text/plain]\n",
            "Saving to: ‚Äòinput.txt‚Äô\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>] 181.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-13 23:30:47 (5.51 MB/s) - ‚Äòinput.txt‚Äô saved [185768/185768]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWdHTo4eocPk",
        "outputId": "23dc0580-9798-4cec-d250-7a70852c6dad"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\n",
            "---\n",
            "\n",
            "Main menu\n",
            "\n",
            "WikipediaThe Free Encyclopedia\n",
            "\n",
            "Search\n",
            "Create account\n",
            "Log in\n",
            "\n",
            "Personal tools\n",
            "Contents  hide\n",
            "(Top)\n",
            "Life and career\n",
            "Toggle Life and career subsection\n",
            "Artistry\n",
            "Toggle Artistry subsection\n",
            "Accolades and achievements\n",
            "Cultural status\n",
            "Toggle Cultural status subsection\n",
            "Wealth\n",
            "Toggle Wealth subsection\n",
            "Discography\n",
            "Filmography\n",
            "Tours\n",
            "See also\n",
            "Footnotes\n",
            "References\n",
            "Toggle References subsection\n",
            "External links\n",
            "Taylor Swift\n",
            "\n",
            "136 languages\n",
            "Article\n",
            "Talk\n",
            "Read\n",
            "View source\n",
            "View history\n",
            "\n",
            "Tools\n",
            " Featured article\n",
            "Page semi-protected\n",
            "From Wikipedia, the free encyclopedia\n",
            "For the album, see Taylor Swift (album).\n",
            "Taylor Swift\n",
            "Portrait of Taylor Swift in a cocktail dress\n",
            "Swift at the 2023 MTV Video Music Awards\n",
            "Born\tTaylor Alison Swift\n",
            "December 13, 1989 (age 34)\n",
            "West Reading, Pennsylvania, US\n",
            "Occupations\n",
            "Singer-songwriter producer director businesswoman actress\n",
            "Years active\t2004‚Äìpresent\n",
            "Works\n",
            "Albumssinglessongsvideosperformance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1\n",
        "Write the BasicTokenizer class, with the following three core functions:\n",
        "\n",
        "\n",
        "*   def train(self, text, vocab_size, verbose=False)\n",
        "*   def encode(self, text)\n",
        "*   def decode(self, ids)\n",
        "\n",
        "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file tests/taylorswift.txt."
      ],
      "metadata": {
        "id": "8QqB35kw66hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids, counts=None):\n",
        "  # needs to store and counts the number of pairs\n",
        "  counts = {} if counts is None else counts\n",
        "  for pair in zip(ids,ids[1:]): # iterates through ids in a pairwise manner\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  return counts\n",
        "\n",
        "def merge(ids, pair,idx):\n",
        "    # idx stores new id values, should be higher than 255 (0-255 BPE)\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids): # iterate through all the ids\n",
        "      # if the id is not the last one and matches the pair inputted\n",
        "      if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "        newids.append(idx) # add to new ids\n",
        "        i += 2\n",
        "      else:\n",
        "        # edge case, pair is at end of text\n",
        "        newids.append(ids[i])\n",
        "        i += 1\n",
        "    return newids"
      ],
      "metadata": {
        "id": "GKhM5pC6Xay-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "# base class for all the tokenizers\n",
        "\n",
        "  def __init__(self):\n",
        "    self.merges = {}\n",
        "    self.pattern = \"\"\n",
        "    self.vocab = self._build_vocab()\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def encode(self, text):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def decode(self, ids):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _build_vocab(self):\n",
        "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "    for (p0, p1), idx in self.merges.items():\n",
        "      vocab[idx] = vocab[p0] + vocab[p1]\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "iI_tz84SXpVW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer(Tokenizer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "      assert vocab_size >= 256\n",
        "      num_merges = vocab_size - 256\n",
        "\n",
        "\n",
        "      ids = list(text.encode(\"utf-8\"))\n",
        "      if num_merges > len(ids):\n",
        "        num_merges = len(ids)-1\n",
        "\n",
        "\n",
        "      vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "      merges = {}\n",
        "      for i in range(num_merges):\n",
        "        stats = get_stats(ids)\n",
        "        pair = max(stats, key=stats.get) #get most common pair\n",
        "        idx = 256 + i\n",
        "        ids = merge(ids, pair, idx)\n",
        "        vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "\n",
        "        merges[pair] = idx\n",
        "\n",
        "        if verbose:\n",
        "          print(f\"merging {pair} into a new token {idx} with {stats[pair]} occurences\")\n",
        "\n",
        "        # save class variables\n",
        "      self.merges = merges # used in encode()\n",
        "      self.vocab = vocab   # used in decode()\n",
        "\n",
        "\n",
        "  def encode(self,text):\n",
        "    # given a string, return list of integers (the tokens)\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    while len(tokens) >= 2: # edge case of inputting only one char\n",
        "      stats = get_stats(tokens)\n",
        "      # min num from merges will return the first pair created\n",
        "      pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "      # subtle: if there are no more merges available, the key will\n",
        "      # result in an inf for every single pair, and the min will be\n",
        "      # just the first pair in the list, arbitrarily\n",
        "      # we can detect this terminating case by a membership check\n",
        "      if pair not in self.merges:\n",
        "        break # nothing else can be merged\n",
        "      idx = self.merges[pair]\n",
        "      tokens = merge(tokens, pair, idx)\n",
        "    return tokens\n",
        "\n",
        "  def decode(self,ids):\n",
        "    # Given a sequence of integers in the range [0, vocab_size], what is the text?\n",
        "    tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd5Ye6Mbm4Wm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Function"
      ],
      "metadata": {
        "id": "ro_lJtbV4b2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"some lines a day\"\n",
        "brokenizer = BasicTokenizer()\n",
        "brokenizer.decode(brokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hOlVapksvHhM",
        "outputId": "a9591467-ea6a-4906-df41-05d837ec2f85"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'some lines a day'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
        "valtext2 = brokenizer.decode(brokenizer.encode(valtext))\n",
        "print(valtext2 == valtext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaB5Rng80F6V",
        "outputId": "70913a8b-d963-46a0-a3c6-182e25c57a56"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brokenizer.train(valtext, 266)\n",
        "brokenizer.merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0C9-zxr0pRO",
        "outputId": "4821b960-897d-4eab-f6c2-246670a0fcb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (116, 104): 257,\n",
              " (97, 114): 258,\n",
              " (97, 110): 259,\n",
              " (111, 110): 260,\n",
              " (105, 110): 261,\n",
              " (100, 32): 262,\n",
              " (32, 257): 263,\n",
              " (111, 102): 264,\n",
              " (99, 111): 265}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with Taylor Swift wiki"
      ],
      "metadata": {
        "id": "kQEu_KVx8K_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 300 - 256"
      ],
      "metadata": {
        "id": "O2YNgzEba2Z2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = BasicTokenizer()\n",
        "# if vocab size is too large, encoutner error\n",
        "tok.train(text, 300)"
      ],
      "metadata": {
        "id": "YbpB6F508Os3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "urXwIGYJFzpO",
        "outputId": "cfbcb8b6-0a62-4348-b4ab-5eb84e824368"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(115, 111): 256,\n",
              " (256, 109): 257,\n",
              " (257, 101): 258,\n",
              " (258, 32): 259,\n",
              " (259, 108): 260,\n",
              " (260, 105): 261,\n",
              " (261, 110): 262,\n",
              " (262, 101): 263,\n",
              " (263, 115): 264,\n",
              " (264, 32): 265,\n",
              " (265, 97): 266,\n",
              " (266, 32): 267,\n",
              " (267, 100): 268,\n",
              " (268, 97): 269,\n",
              " (269, 121): 270}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tok.merges.keys():\n",
        "  print(chr(i[0]), chr(i[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YZOAY4IS9BB6",
        "outputId": "94ae6dcb-0651-4b5d-d357-4c6b35a151ed"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e  \n",
            ",  \n",
            "d  \n",
            ".  \n",
            "r  \n",
            "2 0\n",
            "s  \n",
            "i n\n",
            "o n\n",
            "r i\n",
            "t  \n",
            "t h\n",
            "e ƒÇ\n",
            "ƒÅ ƒÖ\n",
            "a n\n",
            "a r\n",
            "e ƒÑ\n",
            "y  \n",
            "a l\n",
            "ƒã ƒÄ\n",
            "v ƒå\n",
            "w i\n",
            "e r\n",
            "ƒà  \n",
            "ƒï f\n",
            "R e\n",
            "S ƒò\n",
            "o ƒÑ\n",
            "c h\n",
            "ƒç 1\n",
            "o m\n",
            "b ƒê\n",
            "  ƒì\n",
            "a y\n",
            "e n\n",
            "o r\n",
            "ƒí  \n",
            "e m\n",
            ". \n",
            "\n",
            "ƒâ e\n",
            "ƒá g\n",
            "ƒç 2\n",
            "t i\n",
            "ƒ° l\n",
            "\" ƒÉ\n",
            "l l\n",
            "T ƒ´\n",
            "t ƒß\n",
            "ƒ¶  \n",
            "t o\n",
            "ƒÉ ƒô\n",
            "ƒ≤ ƒØ\n",
            "ƒ≥ ƒî\n",
            "ƒÆ ƒõ\n",
            "e s\n",
            "ƒµ ƒö\n",
            "u s\n",
            "r ƒû\n",
            "ƒ• ƒü\n",
            ") ƒÉ\n",
            "A r\n",
            "f ƒπ\n",
            "ƒª \"\n",
            "ƒé ƒÇ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2\n",
        "\n",
        "Convert you BasicTokenizer into a RegexTokenizer, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:"
      ],
      "metadata": {
        "id": "mcycPISv-BRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re"
      ],
      "metadata": {
        "id": "BmbqG-0dD1h9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_SPECIAL_TOKENS = {\n",
        "    '<|endoftext|>': 100257,\n",
        "    '<|fim_prefix|>': 100258,\n",
        "    '<|fim_middle|>': 100259,\n",
        "    '<|fim_suffix|>': 100260,\n",
        "    '<|endofprompt|>': 100276\n",
        "}\n",
        "\n",
        "\n",
        "print(re.findall(GPT4_SPLIT_PATTERN, \"Hello've world123 how's are you!!!?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VimI3zREDH5S",
        "outputId": "e22bae24-c670-4e1b-dfe1-68fc80af2469"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegexTokenizer(Tokenizer):\n",
        "\n",
        "\n",
        "  def __init__(self, pattern=None):\n",
        "    super().__init__()\n",
        "    self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
        "    self.compiled_pattern = re.compile(self.pattern)\n",
        "    self.special_tokens = {}\n",
        "    self.inverse_special_tokens = {}\n",
        "\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    assert vocab_size >= 256\n",
        "    num_merges = vocab_size - 256\n",
        "\n",
        "    # split text into chunks\n",
        "    txt_ch = re.findall(self.compiled_pattern, text)\n",
        "\n",
        "    # process text\n",
        "    ids = [list(ch.encode(\"utf-8\")) for ch in txt_ch]\n",
        "\n",
        "    # if num_merges > len(ids):\n",
        "    #   num_merges = len(ids)-1\n",
        "\n",
        "\n",
        "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "    merges = {}\n",
        "    for i in range(num_merges):\n",
        "      stats = {}\n",
        "      for ch_ids in ids:\n",
        "        get_stats(ch_ids, stats)\n",
        "      pair = max(stats, key=stats.get) #get most common pair\n",
        "      idx = 256 + i\n",
        "      ids = [merge(ch_ids, pair, idx) for ch_ids in ids]\n",
        "      vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "\n",
        "      merges[pair] = idx\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"merging {pair} into a new token {idx} with {stats[pair]} occurences\")\n",
        "\n",
        "    # save class variables\n",
        "    self.merges = merges # used in encode()\n",
        "    self.vocab = vocab   # used in decode()\n",
        "\n",
        "  def decode(self,ids):\n",
        "    # Given a sequence of integers in the range [0, vocab_size], what is the text?\n",
        "    part_bytes = []\n",
        "    for idx in ids:\n",
        "      if idx in self.vocab:\n",
        "        part_bytes.append(self.vocab[idx])\n",
        "      else:\n",
        "        raise ValueError(f\"invalid token id: {idx}\")\n",
        "    text = b\"\".join(part_bytes).decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n",
        "  def register_special_tokens(self, special_tokens):\n",
        "      # special_tokens is a dictionary of str -> int\n",
        "      # example: {\"<|endoftext|>\": 100257}\n",
        "      self.special_tokens = special_tokens\n",
        "      self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
        "\n",
        "  # taken from minibpe\n",
        "\n",
        "  def _encode_chunk(self, text_bytes):\n",
        "    # return the token ids\n",
        "    # let's begin. first, convert all bytes to integers in range 0..255\n",
        "    ids = list(text_bytes)\n",
        "    while len(ids) >= 2:\n",
        "        # find the pair with the lowest merge index\n",
        "        stats = get_stats(ids)\n",
        "        pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "        # subtle: if there are no more merges available, the key will\n",
        "        # result in an inf for every single pair, and the min will be\n",
        "        # just the first pair in the list, arbitrarily\n",
        "        # we can detect this terminating case by a membership check\n",
        "        if pair not in self.merges:\n",
        "            break # nothing else can be merged anymore\n",
        "        # otherwise let's merge the best pair (lowest merge index)\n",
        "        idx = self.merges[pair]\n",
        "        ids = merge(ids, pair, idx)\n",
        "    return ids\n",
        "\n",
        "  def encode_ordinary(self, text):\n",
        "      \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
        "      # split text into chunks of text by categories defined in regex pattern\n",
        "      text_chunks = re.findall(self.compiled_pattern, text)\n",
        "      # all chunks of text are encoded separately, then results are joined\n",
        "      ids = []\n",
        "      for chunk in text_chunks:\n",
        "          chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
        "          chunk_ids = self._encode_chunk(chunk_bytes)\n",
        "          ids.extend(chunk_ids)\n",
        "      return ids\n",
        "\n",
        "  def encode(self, text, allowed_special=\"none_raise\"):\n",
        "      \"\"\"\n",
        "      Unlike encode_ordinary, this function handles special tokens.\n",
        "      allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
        "      if none_raise, then an error is raised if any special token is encountered in text\n",
        "      this is the default tiktoken behavior right now as well\n",
        "      any other behavior is either annoying, or a major footgun\n",
        "      \"\"\"\n",
        "      # decode the user desire with respesct to handling of special tokens\n",
        "      special = None\n",
        "      if allowed_special == \"all\":\n",
        "          special = self.special_tokens\n",
        "      elif allowed_special == \"none\":\n",
        "          special = {}\n",
        "      elif allowed_special == \"none_raise\":\n",
        "          special = {}\n",
        "          assert all(token not in text for token in self.special_tokens)\n",
        "      elif isinstance(allowed_special, set):\n",
        "          special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
        "      else:\n",
        "          raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
        "      if not special:\n",
        "          # shortcut: if no special tokens, just use the ordinary encoding\n",
        "          return self.encode_ordinary(text)\n",
        "      # otherwise, we have to be careful with potential special tokens in text\n",
        "      # we handle special tokens by splitting the text\n",
        "      # based on the occurrence of any exact match with any of the special tokens\n",
        "      # we can use re.split for this. note that surrounding the pattern with ()\n",
        "      # makes it into a capturing group, so the special tokens will be included\n",
        "      special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
        "      special_chunks = re.split(special_pattern, text)\n",
        "      # now all the special characters are separated from the rest of the text\n",
        "      # all chunks of text are encoded separately, then results are joined\n",
        "      ids = []\n",
        "      for part in special_chunks:\n",
        "          if part in special:\n",
        "              # this is a special token, encode it separately as a special case\n",
        "              ids.append(special[part])\n",
        "          else:\n",
        "              # this is an ordinary sequence, encode it normally\n",
        "              ids.extend(self.encode_ordinary(part))\n",
        "      return ids\n"
      ],
      "metadata": {
        "id": "lJ905Doq-JN6"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regtok = RegexTokenizer()\n",
        "regtok.train(text, 272)\n"
      ],
      "metadata": {
        "id": "5bD1a-hcFJCo"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regtok.merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SrFQ6vZF-Nw",
        "outputId": "852e80df-c5c4-419b-a0c6-c944d575041f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 114): 256,\n",
              " (50, 48): 257,\n",
              " (111, 114): 258,\n",
              " (105, 110): 259,\n",
              " (101, 100): 260,\n",
              " (32, 116): 261,\n",
              " (111, 110): 262,\n",
              " (104, 101): 263,\n",
              " (32, 83): 264,\n",
              " (97, 114): 265,\n",
              " (97, 110): 266,\n",
              " (32, 65): 267,\n",
              " (261, 263): 268,\n",
              " (97, 108): 269,\n",
              " (114, 105): 270,\n",
              " (118, 260): 271}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(regtok.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhHWaA9mCohc",
        "outputId": "b1b57896-5ee4-4341-b1af-39293849781b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'er', 257: b'20', 258: b'or', 259: b'in', 260: b'ed', 261: b' t', 262: b'on', 263: b'he', 264: b' S', 265: b'ar', 266: b'an', 267: b' A', 268: b' the', 269: b'al', 270: b'ri', 271: b'ved'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3\n",
        "\n",
        "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both encode and decode, matching tiktoken."
      ],
      "metadata": {
        "id": "zgjPdVp2_yTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recover_merges(mergeable_ranks):\n",
        "    # the `merges` are already the byte sequences in their merged state.\n",
        "    # so we have to recover the original pairings. We can do this by doing\n",
        "    # a small BPE training run on all the tokens, in their order.\n",
        "    # also see https://github.com/openai/tiktoken/issues/60\n",
        "    # also see https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306\n",
        "    merges = {}\n",
        "    for token, rank in mergeable_ranks.items():\n",
        "        if len(token) == 1:\n",
        "            continue # skip raw bytes\n",
        "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
        "        assert len(pair) == 2\n",
        "        # recover the integer ranks of the pair\n",
        "        ix0 = mergeable_ranks[pair[0]]\n",
        "        ix1 = mergeable_ranks[pair[1]]\n",
        "        merges[(ix0, ix1)] = rank\n",
        "\n",
        "    return merges\n",
        "\n",
        "\n",
        "def bpe(mergeable_ranks, token, max_rank):\n",
        "    # helper function used in get_gpt4_merges() to reconstruct the merge forest\n",
        "    parts = [bytes([b]) for b in token]\n",
        "    while True:\n",
        "        min_idx = None\n",
        "        min_rank = None\n",
        "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
        "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "            if rank is not None and (min_rank is None or rank < min_rank):\n",
        "                min_idx = i\n",
        "                min_rank = rank\n",
        "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
        "            break\n",
        "        assert min_idx is not None\n",
        "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
        "    return parts"
      ],
      "metadata": {
        "id": "4bD6oFb3_dh_"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT4Tokenizer(RegexTokenizer):\n",
        "  def __init__(self):\n",
        "    super().__init__(pattern=GPT4_SPLIT_PATTERN)\n",
        "\n",
        "    # get official GPT4 tokenizer and its merges\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    mergeable_ranks = enc._mergeable_ranks\n",
        "\n",
        "    # merges from gpt4\n",
        "    self.merges = recover_merges(mergeable_ranks)\n",
        "\n",
        "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "    # merges is a dict (int, int) --> int\n",
        "    for (p0, p1), idx in self.merges.items():\n",
        "      vocab[idx] = vocab[p0] + vocab[p1]\n",
        "    self.vocab = vocab\n",
        "\n",
        "    self.byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}\n",
        "    self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n",
        "\n",
        "    self.register_special_tokens(GPT4_SPECIAL_TOKENS)\n",
        "\n",
        "  def decode(self, ids):\n",
        "    # unpermute the bytes before decoding\n",
        "    text_bytes = b\"\".join(self.vocab[idx] for idx in ids) # storing all the bytes in a larger byte\n",
        "    text_bytes = bytes(self.inverse_byte_shuffle[i] for i in text_bytes)\n",
        "    text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n",
        "  def _encode_chunk(self, text_bytes):\n",
        "    text_bytes = bytes(self.byte_shuffle[i] for i in text_bytes)\n",
        "    return super()._encode_chunk(text_bytes)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2O8OT4_Y_sxV"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# match this\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\")\n",
        "text = enc.decode(ids) # get the same text back"
      ],
      "metadata": {
        "id": "jNHnwEGeJpJ8"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIwpohkDFZHE",
        "outputId": "43ccfe89-9ce7-4626-c69e-c9bb879ba1e1"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15339, 1917, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 28509, 4513, 57037]\n",
            "hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiktok = GPT4Tokenizer()\n",
        "# tiktok.vocab\n",
        "ids = tiktok.encode(\"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DdYfyTV1DRt8"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)\n",
        "print(tiktok.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlY7R5fjFWlt",
        "outputId": "122e68df-df5f-40be-9194-57018fbb093d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15339, 1917, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 28509, 4513, 57037]\n",
            "hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\"))\n",
        "print(tiktok.encode(\"<|endoftext|>hello world\", allowed_special=\"all\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3tMeHlnFs70",
        "outputId": "32ac2e2f-3c2a-477a-a449-49e420209127"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100257, 15339, 1917]\n",
            "[100257, 15339, 1917]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/karpathy/minbpe/blob/master/exercise.md"
      ],
      "metadata": {
        "id": "1ScO0R4nA-9Y"
      }
    }
  ]
}